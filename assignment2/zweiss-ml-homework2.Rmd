---
title: 'Machine learning for computational linguistics: Assignment 2'
author: "Zarah Weiß"
date: "8 Jun 2016"
output: pdf_document
---

# Task 1

## Task 1.1: Write down the fitted model equation (estimated probability given the predictor)

``` {r echo = FALSE}
dat <- read.csv('/Users/zweiss/Documents/Uni/9_SS16/iscl-s9_ss16-machine_learning/assignment2/data/language-stats.csv')
dat$isGerman <- ifelse(dat$language=='German', TRUE, FALSE)
glm.1 <- glm(isGerman ~ sent_length, family=binomial(link='logit'), dat)
```

$$P(is~German) = \frac{1}{1 + e^{0.4125701 - (-0.0053695 * sentence~length)} }$$

## Task 1.2: Write a brief (with no more than three sentences) interpretation of the coefficients

``` {r echo=FALSE}
summary(glm.1)
```

The intercept has a value of -0.413 with a small standard error. Albeit its comparativeley low value, it is highly significantly different from 0 with p < 0.01.
The predictor sentence length is -0.005 and also highly significant with p < 0.01. The negative polarity indicates that the probability of a sentence being classified as German decreases with increasing sentence length.

### Task 1.3: Report accuracy, precision, recall and F1-score of the fitted model on the training data

``` {r echo=FALSE}
# predict training data
dat.pred.glm1 <- predict(glm.1, newdata = dat, type = 'response')
dat$IsGermanPred.glm1 <- ifelse(dat.pred.glm1 > 0.5, TRUE, FALSE)
# visually inspect results
# plot(x = dat$sent_length, y = dat.pred.glm1, 
#      ylab = "Predicted probability of sentence being German", 
#      xlab = "Sentence length")

# get true positives, false positives, false negatives and true negatives
tp.glm1 <- nrow(dat[dat$isGerman==TRUE & dat$IsGermanPred.glm1==TRUE,])
fp.glm1 <- nrow(dat[dat$isGerman==FALSE & dat$IsGermanPred.glm1==TRUE,])
fn.glm1 <- nrow(dat[dat$isGerman==TRUE & dat$IsGermanPred.glm1==FALSE,])
tn.glm1 <- nrow(dat[dat$isGerman==FALSE & dat$IsGermanPred.glm1==FALSE,])
```

``` {r}
# calculate accuracy
accuracy <- function(tp, fp, tn, fn) {
  return((tp + tn) / (tp + tn + fp + fn))
}
accuracy.glm1 <- accuracy(tp.glm1, fp.glm1, tn.glm1, fn.glm1)
accuracy.glm1

# calculate precision
precision <- function(tp, fp) {
  return(ifelse((tp + fp) > 0, tp / (tp + fp), 0))
}
precision.glm1 <- precision(tp.glm1, fp.glm1)
precision.glm1

# calculate recall
recall <- function(tp, fn) {
  return(ifelse((tp + fn) > 0, tp / (tp + fn), 0))
}
recall.glm1 <- recall(tp.glm1, fn.glm1)
recall.glm1

# calculate f score
f1Score <- function(tp, fp, fn) {
  return(
    ifelse(precision(tp, fp) + recall(tp, fn) > 0, 
           2 * (precision(tp, fp) * recall(tp, fn)) / (precision(tp, fp) + recall(tp, fn)), 
           0))
}
f1.glm1 <- f1Score(tp.glm1, fp.glm1, fn.glm1)
f1.glm1
```

# Task 2

## Task 2.1: Find and report the best threshold value that maximizes the F1-score

The best threshold is 0.35 leading to an f score of 0.552. This is also illustrated in the following plot.

``` {r echo=FALSE}
findBestThreshold <- function(df, predictions, thresholds, PLOT = TRUE) {
  f1 <- c()
  i = 1
  for (t in thresholds) {
    # get prediction labels based on new threshold t
    df$predicted <- ifelse(predictions > t, TRUE, FALSE)
    # get true positives, false positives, false negatives and true negatives
    tp.tmp <- nrow(df[df$expected==TRUE & df$predicted==TRUE,])
    fp.tmp <- nrow(df[df$expected==FALSE & df$predicted==TRUE,])
    fn.tmp <- nrow(df[df$expected==TRUE & df$predicted==FALSE,])
    # get f1
    f1[i] <- f1Score(tp.tmp, fp.tmp, fn.tmp)
    i = i + 1
  }
  if(PLOT) {
    plot(thresholds, f1, type="b", xlab="Threshold", ylab="F1 Score")
  }
  # return best value
  tBest = -1
  for (j in 1:length(thresholds)) {
    if(f1[j] == max(f1)) {
      tBest = thresholds[j]
    }
  }
  return(tBest)
}

expected <- dat$isGerman
# find best threshold
tBest <- findBestThreshold(data.frame(expected), dat.pred.glm1, c(0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.45, 0.5))
```


## Task 2.2: Write down the discriminant function (the function f(X) whose value is positive for the positive instances (sentences in German) and negative for the negative instances)

    f(x) = 0, if x < 0.35, else f(x) = 1

Or in R:

``` {r}
dat$IsGermanPred.glm1.bestThreshold <- ifelse(dat.pred.glm1 > tBest, TRUE, FALSE)
```

## Task 2.3: Report the accuracy, precision, recall and F1-score at the best threshold value

``` {r echo=FALSE}
# get the new true positives, false positives, false negatives and true negatives
tp.glm1.35 <- nrow(dat[dat$isGerman==TRUE & dat$IsGermanPred.glm1.bestThreshold==TRUE,])
fp.glm1.35 <- nrow(dat[dat$isGerman==FALSE & dat$IsGermanPred.glm1.bestThreshold==TRUE,])
fn.glm1.35 <- nrow(dat[dat$isGerman==TRUE & dat$IsGermanPred.glm1.bestThreshold==FALSE,])
tn.glm1.35 <- nrow(dat[dat$isGerman==FALSE & dat$IsGermanPred.glm1.bestThreshold==FALSE,])
```

``` {r}
# calculate new measures
accuracy.glm1.35 <- accuracy(tp.glm1.35, fp.glm1.35, tn.glm1.35, fn.glm1.35)
accuracy.glm1.35
precision.glm1.35 <- precision(tp.glm1.35, fp.glm1.35)
precision.glm1.35
recall.glm1.35 <- recall(tp.glm1.35, fn.glm1.35)
recall.glm1.35
f1.glm1.35 <- f1Score(tp.glm1.35, fp.glm1.35, fn.glm1.35)
f1.glm1.35
```

# Task 3

``` {r echo=FALSE}
# make a data set containing of only the relative POS frequencies and sentence length
dat.reduced <- dat[,-c(16, 18, 20, 21, 22)]
str(dat.reduced)

# train model with all features (only POS and sentence length left)
glm.2 <- glm(isGerman ~ ., family=binomial(link='logit'), dat.reduced)
```

## Task 3.1: This time, besides the sentence length, use 15 additional predictors that indicate the relative frequencies of POS unigrams within the sentence. Evaluate your model with probability threshold of 0.5,

``` {r}
summary(glm.2)
```

The intercept is negative again (as with the previous model), but has a more extreme value, namely -17.061. It significantly differs from zero, with p < 0.01. As for sentence öength, the feature is still significant, however, it is not negatively correlated with a sentence being German anymore, i.e. the longer a sentence, the more likely it is to be German. This might be due to the fact that the sentences from the German data set are longer (mean = 18.76) than the English ones (mean = 15.33), but shorter than the Japanese ones (mean = 26.78), see below.

The POS features are mostly highly significant, too, i.e. make good predictors for language, and positively correlated with a sentence being German. Interestingly, the only negative coefficient, namely the relative frequency of the SCONJ POS tag, is also not predictive with p = 0.477. The other not significant predictor is the relative frequency of X, for which NA is returned. This indicates a high correlation of X with some other predictor.

Also, the AIC droped from 56,157 to 38,229 indicating that the second model is much better suited for the identification of German in this data set.

``` {r}
# Sanity check: sentence length across languages
summary(dat$sent_length[dat$language=='English'])
summary(dat$sent_length[dat$language=='German'])
summary(dat$sent_length[dat$language=='Japanese'])
```

## Task 3.2 ... and report accuracy, precision, recall and F1-score values.

``` {r echo=FALSE, warning=FALSE}
# predict training data
dat.pred.glm2 <- predict(glm.2, newdata = dat.reduced, type = 'response')
dat$IsGermanPred.glm2 <- ifelse(dat.pred.glm2 > 0.5, TRUE, FALSE)

# get true positives, false positives, false negatives and true negatives
tp.glm2 <- nrow(dat[dat$isGerman==TRUE & dat$IsGermanPred.glm2==TRUE,])
fp.glm2 <- nrow(dat[dat$isGerman==FALSE & dat$IsGermanPred.glm2==TRUE,])
fn.glm2 <- nrow(dat[dat$isGerman==TRUE & dat$IsGermanPred.glm2==FALSE,])
tn.glm2 <- nrow(dat[dat$isGerman==FALSE & dat$IsGermanPred.glm2==FALSE,])
```

```{r}
# calculate accuracy, precision, recall, and fscore
accuracy.glm2 <- accuracy(tp.glm2, fp.glm2, tn.glm2, fn.glm2)
accuracy.glm2
precision.glm2 <- precision(tp.glm2, fp.glm2)
precision.glm2
recall.glm2 <-recall(tp.glm2, fn.glm2)
recall.glm2
f1.glm2 <- f1Score(tp.glm2, fp.glm2, fn.glm2) 
f1.glm2
```

## Task 3.3: Does this model suffer from the class imbalance problem equally?

No it doesn't. This can be seen at the results for accuracy, precision, recall and F1 being quite similar. This might be due to the fact, that we have a number of good predictors now, that can identify the classes not just by a majority vote.

# Task 4

## Task 4.1: Fit two separate models to the complete data, one with L1, the other one with L2 regularization. For both models, use the regularization parameter Lambda = 50.

see R code

``` {r echo=FALSE, message=FALSE}
library(LiblineaR)
# 4.1 Fit two separate models to the complete data, one with L1, the other one with L2 regularization. For both 
# models, use the regularization parameter = 50.
dat.reduced2 <- dat[, -c(16, 18, 19, 20, 21, 22)]

linM.l2 <- LiblineaR(data = dat.reduced2, target = dat$language, type = 0, cost = 50)
linM.l1 <- LiblineaR(data = dat.reduced2, target = dat$language, type = 6, cost = 50)
```

## Task 4.2: Briefly explain the differences between the coefficient values.

Regularisation is used to avoid overfitting. This is an issue we are quite exposed to, as we use our training data also as testing data. Model linM.l1 uses a L1 regularisation to constrain the parameter space, that is, it adds the L1 norm multiplied with Lambda = 50 to the cost function. Model linM.l2 uses a L2 regularisation, that is, it adds the Euclidean and not the Manhattan distance to the cost function. 

``` {r}
linM.l1
linM.l2
```

In principle, the coefficients of both models resemble each other. However, the L2 regularization returns less extreme values. So when the L1 regularised model returns a coefficient of -10.688 for ADJ in Japanese sentences, the L2 regularised model returns a coefficient of -5.707. Something similar can be observed for DET, where for German the L1 regularised model returns a coefficient of 14.547 and the L2 regularised model a coefficient of 7.122. As the L2 regularisation may be viewed as equivalent to a normal prior centered around the mean, if we view it in a Bayesian setting, this is not surprising: the constraint on the parameter space is simply centered around zero leading to coefficients more closely to zero compared to the L1 regularisation. However, even if the coefficients take less extreme values centered more closely around zero in the L2 regularised model, within a parameter both models keep the same relation of the language specific coefficients, i.e. for PROPN, Japanese has in both models the most extreme negative slope compared to the slope for German and English, even though the absolute values differ across the models.

## Task 4.3: Calculate and compare accuracy of both L1 and L2 regularized models.

``` {r echo=FALSE}
# predict training data
dat.pred.linM.l2 <- predict(linM.l2, newx = dat.reduced2, proba = TRUE)
dat.pred.linM.l1 <- predict(linM.l1, newx = dat.reduced2)

# get true positives, false positives, false negatives and true negatives
tp.l2.english <- nrow(dat[dat$language=='English' & dat.pred.linM.l2$predictions=='English',])
fp.l2.english <- nrow(dat[dat$language!='English' & dat.pred.linM.l2$predictions=='English',])
fn.l2.english <- nrow(dat[dat$language=='English' & dat.pred.linM.l2$predictions!='English',])
tn.l2.english <- nrow(dat[dat$language!='English' & dat.pred.linM.l2$predictions!='English',])
tp.l2.german <- nrow(dat[dat$language=='German' & dat.pred.linM.l2$predictions=='German',])
fp.l2.german <- nrow(dat[dat$language!='German' & dat.pred.linM.l2$predictions=='German',])
fn.l2.german <- nrow(dat[dat$language=='German' & dat.pred.linM.l2$predictions!='German',])
tn.l2.german <- nrow(dat[dat$language!='German' & dat.pred.linM.l2$predictions!='German',])
tp.l2.japanese <- nrow(dat[dat$language=='Japanese' & dat.pred.linM.l2$predictions=='Japanese',])
fp.l2.japanese <- nrow(dat[dat$language!='Japanese' & dat.pred.linM.l2$predictions=='Japanese',])
fn.l2.japanese <- nrow(dat[dat$language=='Japanese' & dat.pred.linM.l2$predictions!='Japanese',])
tn.l2.japanese <- nrow(dat[dat$language!='Japanese' & dat.pred.linM.l2$predictions!='Japanese',])

tp.l1.english <- nrow(dat[dat$language=='English' & dat.pred.linM.l1$predictions=='English',])
fp.l1.english <- nrow(dat[dat$language!='English' & dat.pred.linM.l1$predictions=='English',])
fn.l1.english <- nrow(dat[dat$language=='English' & dat.pred.linM.l1$predictions!='English',])
tn.l1.english <- nrow(dat[dat$language!='English' & dat.pred.linM.l1$predictions!='English',])
tp.l1.german <- nrow(dat[dat$language=='German' & dat.pred.linM.l1$predictions=='German',])
fp.l1.german <- nrow(dat[dat$language!='German' & dat.pred.linM.l1$predictions=='German',])
fn.l1.german <- nrow(dat[dat$language=='German' & dat.pred.linM.l1$predictions!='German',])
tn.l1.german <- nrow(dat[dat$language!='German' & dat.pred.linM.l1$predictions!='German',])
tp.l1.japanese <- nrow(dat[dat$language=='Japanese' & dat.pred.linM.l1$predictions=='Japanese',])
fp.l1.japanese <- nrow(dat[dat$language!='Japanese' & dat.pred.linM.l1$predictions=='Japanese',])
fn.l1.japanese <- nrow(dat[dat$language=='Japanese' & dat.pred.linM.l1$predictions!='Japanese',])
tn.l1.japanese <- nrow(dat[dat$language!='Japanese' & dat.pred.linM.l1$predictions!='Japanese',])
```

``` {r}
# calculate accuracy for model using L1 regularisation
accuracy.l1.overall <- accuracy(tp.l1.english + tp.l1.german + tp.l1.japanese, 
                                fp.l1.english + fp.l1.german + tp.l1.japanese, 
                                tn.l1.english + tn.l1.german + tn.l1.german, 
                                fn.l1.english + fn.l2.german + fn.l1.japanese)
accuracy.l1.overall

# calculate accuracy for model using L2 regularisation
accuracy.l2.overall <- accuracy(tp.l2.english + tp.l2.german + tp.l2.japanese, 
                                fp.l2.english + fp.l2.german + tp.l2.japanese, 
                                tn.l2.english + tn.l2.german + tn.l2.german, 
                                fn.l2.english + fn.l2.german + fn.l2.japanese)
accuracy.l2.overall
```

The accuarcy for both models is rather high and very similar: for the L2 regularised model it is 0.77, for the L1 regularised model it is slightly higher with 0.78.

# Task 4.4: Tabulate the confusion matrix of the L2-regularized model you fit in the previous step.

``` {r echo=FALSE}
# actually english, predicted as: english, german, japanese
exp.english <- c(nrow(dat[dat$language=='English' & dat.pred.linM.l2$predictions=='English',]),
             nrow(dat[dat$language=='English' & dat.pred.linM.l2$predictions=='German',]),
             nrow(dat[dat$language=='English' & dat.pred.linM.l2$predictions=='Japanese',]))

# actually german, predicted as: english, german, japanese
exp.german <- c(nrow(dat[dat$language=='German' & dat.pred.linM.l2$predictions=='English',]),
             nrow(dat[dat$language=='German' & dat.pred.linM.l2$predictions=='German',]),
             nrow(dat[dat$language=='German' & dat.pred.linM.l2$predictions=='Japanese',]))

# actually englisjapaneseh, predicted as: english, german, japanese
exp.japanese <- c(nrow(dat[dat$language=='Japanese' & dat.pred.linM.l2$predictions=='English',]),
             nrow(dat[dat$language=='Japanese' & dat.pred.linM.l2$predictions=='German',]),
             nrow(dat[dat$language=='Japanese' & dat.pred.linM.l2$predictions=='Japanese',]))

conf.matrix <- rbind(exp.english, exp.german, exp.japanese)
colnames(conf.matrix) <- c('pred.english', 'pred.german', 'pred.japanese')
conf.matrix
```

# Task 5: Using the same model in exercise 4 with L2 regularization, evaluate the model accuracy using 10-fold cross validation, and report the average accuracy and its standard error.

``` {r, echo=FALSE}
# cross validation

start <- c(0, 0, 0)
# dat is sorted: first all English, then all German, then Japanese, i.e. German starts after the last English row, etc.
end <- c(0, nrow(dat[dat$language=='English',]), nrow(dat[dat$language=='English',])+nrow(dat[dat$language=='German',]))
chunk <- c(trunc(nrow(dat[dat$language=='English',])/10), trunc(nrow(dat[dat$language=='German',])/10), 
           trunc(nrow(dat[dat$language=='Japanese',])/10))

accuracies <- c()
for (k in 1:10) {

  # update indices for chunks for each language in the data
  for (l in 1:3) {
    start[l] <- end[l] + 1
    # if it is the last chunk, just take the rest
    if (k == 10 & l == 3) {
      end <- c(nrow(dat[dat$language=='English',]), 
               nrow(dat[dat$language=='English',])+nrow(dat[dat$language=='German',]), 
               nrow(dat[dat$language=='English',])+nrow(dat[dat$language=='German',])+nrow(dat[dat$language=='Japanese',]))
    } else {
      end[l] <- start[l] + chunk[l] -1
    }
  }
  
#   print(start)
#   print(end)
#   print(ranges)
  
  # set up test and training
  dat.train <- dat.reduced2[-c(start[1]:end[1], start[2]:end[2], start[3]:end[3]),]
  lang.train <- dat$language[-c(start[1]:end[1], start[2]:end[2], start[3]:end[3])]
  
  dat.test <- dat.reduced2[c(start[1]:end[1], start[2]:end[2], start[3]:end[3]),]
  expected <- dat$language[c(start[1]:end[1], start[2]:end[2], start[3]:end[3])]
  lang.test <- as.data.frame(expected)
   
  # build model
  linM.l2.tmp <- LiblineaR(data = dat.train, target = lang.train, type = 0, cost = 50)
  # test model
  linM.l2.tmp.pred <- predict(linM.l2.tmp, newx = dat.test, proba = TRUE)
  lang.test$predicted <- linM.l2.tmp.pred$predictions
  
  # get accuracy
  tp.e <- nrow(lang.test[lang.test$expected=='English' & lang.test$predicted=='English',])
  tp.g <- nrow(lang.test[lang.test$expected=='German' & lang.test$predicted=='German',])
  tp.j <- nrow(lang.test[lang.test$expected=='Japanese' & lang.test$predicted=='Japanese',])
  tp.tmp <- tp.e + tp.g + tp.j
  
  fp.e <- nrow(lang.test[lang.test$expected!='English' & lang.test$predicted=='English',])
  fp.g <- nrow(lang.test[lang.test$expected!='German' & lang.test$predicted=='German',])
  fp.j <- nrow(lang.test[lang.test$expected!='Japanese' & lang.test$predicted=='Japanese',])
  fp.tmp <- fp.e + fp.g + fp.j
  
  tn.e <- nrow(lang.test[lang.test$expected!='English' & lang.test$predicted!='English',])
  tn.g <- nrow(lang.test[lang.test$expected!='German' & lang.test$predicted!='German',])
  tn.j <- nrow(lang.test[lang.test$expected!='Japanese' & lang.test$predicted!='Japanese',])
  tn.tmp <- tn.e + tn.g + tn.j
  
  fn.e <- nrow(lang.test[lang.test$expected=='English' & lang.test$predicted!='English',])
  fn.g <- nrow(lang.test[lang.test$expected=='German' & lang.test$predicted!='German',])
  fn.j <- nrow(lang.test[lang.test$expected=='Japanese' & lang.test$predicted!='Japanese',])
  fn.tmp <- fn.e + fn.g + fn.j
    
  accuracies[k] <- accuracy(tp.tmp, fp.tmp, tn.tmp, fn.tmp)
}

avgAcc <- mean(accuracies)
stdErr <- sd(accuracies) / sqrt(length(accuracies))
```

The mean accuracy:
``` {r}
avgAcc
```

The standard error:
``` {r}
stdErr
```

